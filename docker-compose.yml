services:
  n8n:
    build: .
    image: docker.n8n.io/n8nio/n8n:latest
    container_name: n8n
    restart: "no"
    ports:
      - "5678:5678"
    environment:
      - N8N_PERSONALIZATION_ENABLED=false
      - N8N_EMAIL_MODE=smtp
      - N8N_USER_ID=1
      - GENERIC_TIMEZONE=America/Fortaleza
      - TZ=America/Fortaleza
      - N8N_BLOCK_FILE_ACCESS_TO_N8N_FILES=false
      - NODE_FUNCTION_ALLOW_BUILTIN=*
      - NODE_FUNCTION_ALLOW_EXTERNAL=*
      # desabilita o login
      # - N8N_USER_MANAGEMENT_DISABLED=true
      - N8N_USER_MANAGEMENT_DISABLED=false
      - NODE_OPTIONS=--max-old-space-size=4096
      - N8N_BLOCK_FS_WRITE_ACCESS=false
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=false
      - N8N_BLOCK_FS_READ_ACCESS=false
      - N8N_ALLOWED_BINARY_DATA_PATH=/files,/home/node/.n8n-files
      # - N8N_DEFAULT_BINARY_DATA_MODE=database
      - N8N_DEFAULT_BINARY_DATA_MODE=filesystem
      - N8N_COMMUNITY_PACKAGES_ENABLED=true
      - N8N_ENABLE_EXECUTE_COMMAND=true
      - N8N_RUNNERS_ENABLED=false
      - NODES_EXCLUDE=
    volumes:
      - ./arquivos_docker/n8n_data:/home/node/.n8n
      - ./arquivos_docker/arquivos_n8n:/home/node/.n8n-files
    depends_on:
      qdrant:
        condition: service_started
      ollama:
        condition: service_started
      redis:
        condition: service_started
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - n8n-network
      
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: "no"
    ports:
      - "6333:6333"
    volumes:
      - ./arquivos_docker/qdrant_data:/qdrant/storage
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - n8n-network

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: "no"
    ports:
      - "11434:11434"
    volumes:
      - E:/programas/ia/.ollama:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_NUM_GPU=99
      - OLLAMA_KEEP_ALIVE=24h
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - n8n-network
    entrypoint: 
      - "/bin/bash"
      - "-c"
      - |
        ollama serve & 
        sleep 10
        echo "Turbinando modelos para RTX 4090..."

        # Qwen3-Coder (30B): 40k é o limite seguro para manter 100% na VRAM
        ollama create qwen3-max -f <(echo "FROM qwen3-coder:latest" && echo "PARAMETER num_ctx 40960")

        # Llama 3.1 (8B): É leve, podemos colocar 64k de contexto com folga
        ollama create llama-max -f <(echo "FROM llama3.1:latest" && echo "PARAMETER num_ctx 65536")

        # GLM 4.7 Flash (19GB): Vamos colocar 32k (ele é pesado como o Qwen)
        ollama create glm-max -f <(echo "FROM glm-4.7-flash:latest" && echo "PARAMETER num_ctx 32768")

        echo "Modelos prontos: qwen3-max, llama-max e glm-max"
        wait

  redis:
    image: redis:6-alpine
    container_name: redis
    restart: "no"
    command: ["redis-server", "--bind", "0.0.0.0", "--protected-mode", "no", "--appendonly", "yes"]
    ports:
      - "6379:6379"
    volumes:
      - ./arquivos_docker/redis_data:/data
    networks:
      - n8n-network

networks:
  n8n-network:
    driver: bridge